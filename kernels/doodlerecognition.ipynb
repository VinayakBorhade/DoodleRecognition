{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['shufflecsvs', 'quickdraw-doodle-recognition', 'shuffletestcsv', 'keraspretrainedmodels']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "from glob import glob\n",
    "import re\n",
    "import ast\n",
    "import cv2\n",
    "import csv\n",
    "import time\n",
    "import ast\n",
    "import urllib\n",
    "from PIL import Image, ImageDraw\n",
    "from tqdm import tqdm\n",
    "from dask import bag, threaded\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as pltc\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from dask import bag, threaded\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from tensorflow.keras.metrics import categorical_accuracy, top_k_categorical_accuracy, categorical_crossentropy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications.nasnet import NASNetMobile\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from collections import Counter \n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "BASE_SIZE = 256\n",
    "DP_DIR = \"../input/shufflecsvs/shufflecsv/\"\n",
    "INPUT_DIR = '../input/quickdraw-doodle-recognition/'\n",
    "TEST_DIR=\"../input/shuffletestcsv/shuffletestcsvs/\"\n",
    "NCSVS = 4\n",
    "NCATS = 340\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_k2.csv.gz', 'train_k3.csv.gz', 'train_k0.csv.gz', 'train_k1.csv.gz']\n",
      "['test_k0.csv.gz', 'test_k1.csv.gz', 'test_k2.csv.gz', 'test_k3.csv.gz']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(\"../input/shufflecsvs/shufflecsv\"))\n",
    "print(os.listdir(\"../input/shuffletestcsv/shuffletestcsvs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": false,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count:  144722\n",
      "['shufflecsvs', 'quickdraw-doodle-recognition', 'shuffletestcsv', 'keraspretrainedmodels']\n",
      "df_arr:  <pandas.io.parsers.TextFileReader object at 0x7f1d2376bda0>\n",
      "df.head():    countrycode  ...     word\n",
      "0          US  ...    apple\n",
      "1          RU  ...    apple\n",
      "2          GB  ...    apple\n",
      "3          US  ...    apple\n",
      "4          TH  ...    apple\n",
      "\n",
      "[5 rows x 6 columns]\n",
      "df.columns:  Index(['countrycode', 'drawing', 'key_id', 'recognized', 'timestamp', 'word'], dtype='object')\n",
      "printing df_arr ----- \n",
      "i:  0  :  144\n",
      "i:  1  :  144\n",
      "i:  2  :  144\n",
      "i:  3  :  144\n",
      "i:  4  :  144\n",
      "i:  5  :  144\n",
      "i:  6  :  144\n",
      "i:  7  :  144\n",
      "i:  8  :  144\n",
      "i:  9  :  144\n",
      "i:  10  :  144\n",
      "i:  11  :  144\n",
      "i:  12  :  144\n",
      "i:  13  :  144\n",
      "i:  14  :  144\n",
      "i:  15  :  144\n",
      "i:  16  :  144\n",
      "i:  17  :  144\n",
      "i:  18  :  144\n",
      "i:  19  :  144\n",
      "i:  20  :  144\n",
      "i:  21  :  144\n",
      "i:  22  :  144\n",
      "i:  23  :  144\n",
      "i:  24  :  144\n",
      "i:  25  :  144\n",
      "i:  26  :  144\n",
      "i:  27  :  144\n",
      "i:  28  :  144\n",
      "i:  29  :  144\n",
      "i:  30  :  144\n",
      "i:  31  :  144\n",
      "i:  32  :  144\n",
      "i:  33  :  144\n",
      "i:  34  :  144\n",
      "i:  35  :  144\n",
      "i:  36  :  144\n",
      "i:  37  :  144\n",
      "i:  38  :  144\n",
      "i:  39  :  144\n",
      "i:  40  :  144\n",
      "i:  41  :  144\n",
      "i:  42  :  144\n",
      "i:  43  :  144\n",
      "i:  44  :  144\n",
      "i:  45  :  144\n",
      "i:  46  :  144\n",
      "i:  47  :  144\n",
      "i:  48  :  144\n",
      "i:  49  :  144\n",
      "i:  50  :  144\n",
      "i:  51  :  144\n",
      "i:  52  :  144\n",
      "i:  53  :  144\n",
      "i:  54  :  144\n",
      "i:  55  :  144\n",
      "i:  56  :  144\n",
      "i:  57  :  144\n",
      "i:  58  :  144\n",
      "i:  59  :  144\n",
      "i:  60  :  144\n",
      "i:  61  :  144\n",
      "i:  62  :  144\n",
      "i:  63  :  144\n",
      "i:  64  :  144\n",
      "i:  65  :  144\n",
      "i:  66  :  144\n",
      "i:  67  :  144\n",
      "i:  68  :  144\n",
      "i:  69  :  144\n",
      "i:  70  :  144\n",
      "i:  71  :  144\n",
      "i:  72  :  144\n",
      "i:  73  :  144\n",
      "i:  74  :  144\n",
      "i:  75  :  144\n",
      "i:  76  :  144\n",
      "i:  77  :  144\n",
      "i:  78  :  144\n",
      "i:  79  :  144\n",
      "i:  80  :  144\n",
      "i:  81  :  144\n",
      "i:  82  :  144\n",
      "i:  83  :  144\n",
      "i:  84  :  144\n",
      "i:  85  :  144\n",
      "i:  86  :  144\n",
      "i:  87  :  144\n",
      "i:  88  :  144\n",
      "i:  89  :  144\n",
      "i:  90  :  144\n",
      "i:  91  :  144\n",
      "i:  92  :  144\n",
      "i:  93  :  144\n",
      "i:  94  :  144\n",
      "i:  95  :  144\n",
      "i:  96  :  144\n",
      "i:  97  :  144\n",
      "i:  98  :  144\n",
      "i:  99  :  144\n",
      "i:  100  :  144\n",
      "i:  101  :  144\n",
      "i:  102  :  144\n",
      "i:  103  :  144\n",
      "i:  104  :  144\n",
      "i:  105  :  144\n",
      "i:  106  :  144\n",
      "i:  107  :  144\n",
      "i:  108  :  144\n",
      "i:  109  :  144\n",
      "i:  110  :  144\n",
      "i:  111  :  144\n",
      "i:  112  :  144\n",
      "i:  113  :  144\n",
      "i:  114  :  144\n",
      "i:  115  :  144\n",
      "i:  116  :  144\n",
      "i:  117  :  144\n",
      "i:  118  :  144\n",
      "i:  119  :  144\n",
      "i:  120  :  144\n",
      "i:  121  :  144\n",
      "i:  122  :  144\n",
      "i:  123  :  144\n",
      "i:  124  :  144\n",
      "i:  125  :  144\n",
      "i:  126  :  144\n",
      "i:  127  :  144\n",
      "i:  128  :  144\n",
      "i:  129  :  144\n",
      "i:  130  :  144\n",
      "i:  131  :  144\n",
      "i:  132  :  144\n",
      "i:  133  :  144\n",
      "i:  134  :  144\n",
      "i:  135  :  144\n",
      "i:  136  :  144\n",
      "i:  137  :  144\n",
      "i:  138  :  144\n",
      "i:  139  :  144\n",
      "i:  140  :  144\n",
      "i:  141  :  144\n",
      "i:  142  :  144\n",
      "i:  143  :  144\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "#print(os.listdir(\"../input\"))\n",
    "count=0\n",
    "count=len(pd.read_csv(\"../input/quickdraw-doodle-recognition/train_simplified/apple.csv\"))\n",
    "\n",
    "print(\"count: \", count)\n",
    "print(os.listdir(\"../input\"))\n",
    "df_arr=pd.read_csv(\"../input/quickdraw-doodle-recognition/train_simplified/apple.csv\", iterator=True, chunksize=count//1000)\n",
    "print(\"df_arr: \",df_arr)\n",
    "df=pd.read_csv(\"../input/quickdraw-doodle-recognition/train_simplified/apple.csv\")\n",
    "print(\"df.head(): \",df.head())\n",
    "print(\"df.columns: \", df.columns)\n",
    "print(\"printing df_arr ----- \")\n",
    "ii=0\n",
    "for i in range(count//1000):\n",
    "    print(\"i: \",i,\" : \", len(pd.DataFrame(df_arr.get_chunk(count//1000)) ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "sum1:  120000\n",
      "validation data dimensions:  10114  ,  (10114, 4)\n",
      "total_len_train 84000.0\n",
      "total_len_valid 36000.0\n"
     ]
    }
   ],
   "source": [
    "#messageData.py modified\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "class messageData:\n",
    "    def __init__(self):\n",
    "        self.total_len_train=0\n",
    "        self.total_len_valid=0\n",
    "        self.files=['apple.csv', 'banana.csv', 'circle.csv', 'pineapple.csv']\n",
    "        self.class_to_idx = {v.split('.')[0]: k for k, v in enumerate(self.files)}\n",
    "        self.idx_to_class = {v: k for k, v in self.class_to_idx.items()}\n",
    "    def setLengths(self, folder=DP_DIR, ks=4):\n",
    "        self.total_len_train=0\n",
    "        self.total_len_valid=0\n",
    "        self.folder=folder\n",
    "        flag=True\n",
    "        sum1=0\n",
    "        for k in np.random.permutation(ks):\n",
    "            filename = os.path.join(DP_DIR, 'train_k{}.csv.gz'.format(k))\n",
    "            name = filename.split('.')[0]\n",
    "            samples=pd.read_csv(filename)#np.genfromtxt(fullpath, delimiter=',')\n",
    "            print(type(samples))\n",
    "            sum1=sum1+samples['drawing'].shape[0]\n",
    "            s_temp=int(samples.shape[0])\n",
    "            self.total_len_train+= s_temp*0.7\n",
    "            self.total_len_valid+= s_temp*0.3\n",
    "        print(\"sum1: \", sum1)\n",
    "        self.sum1 = sum1\n",
    "    \n",
    "    def draw_cv2(self, raw_strokes, size=256, lw=6):\n",
    "        img = np.zeros((BASE_SIZE, BASE_SIZE), np.uint8)\n",
    "        for stroke in raw_strokes:\n",
    "            for i in range(len(stroke[0]) - 1):\n",
    "                _ = cv2.line(img, (stroke[0][i], stroke[1][i]), (stroke[0][i + 1], stroke[1][i + 1]), 255, lw)\n",
    "        if size != BASE_SIZE:\n",
    "            return cv2.resize(img, (size, size))\n",
    "        else:\n",
    "            return img\n",
    "#----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    def generator(self, folder=DP_DIR, batch_size=128, size=80, lw=6, num_classes=4, ks=4):\n",
    "        while True:\n",
    "            for k in np.random.permutation(ks):\n",
    "                filename = os.path.join(DP_DIR, 'train_k{}.csv.gz'.format(k))\n",
    "                for df in pd.read_csv(filename, chunksize=batch_size):\n",
    "                    df['drawing'] = df['drawing'].apply(ast.literal_eval)\n",
    "                    x = np.zeros((len(df), size, size))\n",
    "                    for i, raw_strokes in enumerate(df.drawing.values):\n",
    "                        x[i] = self.draw_cv2(raw_strokes, size=size, lw=lw)\n",
    "                    x = x / 255.\n",
    "                    x = x.reshape((len(df), size*size)).astype(np.float32)\n",
    "                    y = tf.keras.utils.to_categorical(df.y, num_classes=num_classes)\n",
    "                    yield x, y\n",
    "                    \n",
    "    def df_to_image_array(self, df, size=80, lw=6):\n",
    "        df['drawing'] = df['drawing'].apply(ast.literal_eval)\n",
    "        x = np.zeros((len(df), size, size))\n",
    "        for i, raw_strokes in enumerate(df.drawing.values):\n",
    "            x[i] = self.draw_cv2(raw_strokes, size=size, lw=lw)\n",
    "        x = x / 255.\n",
    "        x = x.reshape((len(df), size*size)).astype(np.float32)\n",
    "        return x\n",
    "    def makeValidationData(self, TEST_DIR=TEST_DIR, NCSVS=4, NCATS=4, size=80):\n",
    "        valid_df=pd.read_csv(os.path.join(TEST_DIR, 'test_k{}.csv.gz'.format(0)) )\n",
    "        self.x_valid = self.df_to_image_array(valid_df, size)\n",
    "        self.y_valid = tf.keras.utils.to_categorical(valid_df.y, num_classes=NCATS)\n",
    "        print(\"validation data dimensions: \", self.x_valid.shape[0], \" , \", self.y_valid.shape)\n",
    "        \n",
    "data=messageData()\n",
    "data.setLengths()\n",
    "data.makeValidationData()\n",
    "print(\"total_len_train\", data.total_len_train)\n",
    "print(\"total_len_valid\", data.total_len_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading the libraries\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "sum1:  120000\n",
      "validation data dimensions:  10114  ,  (10114, 4)\n",
      "length of train_samples:  84000.0\n",
      "length of validation_samples:  36000.0\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               640100    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 404       \n",
      "=================================================================\n",
      "Total params: 640,504\n",
      "Trainable params: 640,504\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:53: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:53: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., validation_data=(array([[0..., epochs=3, steps_per_epoch=120000, verbose=1, validation_steps=0)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      " 10217/120000 [=>............................] - ETA: 1:20:31 - loss: 0.2896 - acc: 0.9190"
     ]
    }
   ],
   "source": [
    "#baseline.py modified\n",
    "import numpy\n",
    "import time\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten, Input\n",
    "from keras.models import Model\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import TensorBoard\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.optimizers import Adam\n",
    "#import massageData\n",
    "\n",
    "CLASS_NUM = 4\n",
    "BITMAP_DIM=80*80\n",
    "BATCH_SIZE=128\n",
    "EPOCHS=2\n",
    "STEPS = 1000\n",
    "model=None\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    #model=Sequential()\n",
    "    #model.add(Dense(1, input_dim=BITMAP_DIM, activation='sigmoid'))\n",
    "    #model.add(Dense(CLASS_NUM, activation='softmax'))\n",
    "    input_X = Input(shape=(BITMAP_DIM,))\n",
    "    x = Dense(100)(input_X)\n",
    "    x = Dense(CLASS_NUM, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=input_X, outputs=x)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=5e-5), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    print (\"Done loading the libraries\")\n",
    "    t0 = time.time()\n",
    "    seed = 7\n",
    "    numpy.random.seed(seed)\n",
    "    data = messageData()\n",
    "    data.setLengths()\n",
    "    data.makeValidationData()\n",
    "    #generators\n",
    "    #train_generator = data.generator(folder=DP_DIR, batch_size=BATCH_SIZE) \n",
    "    train_generator=data.generator( batch_size=BATCH_SIZE, ks=range(NCSVS - 1))\n",
    "    #validation_generator = data.generator('../input/train_simplified/', isTrain=False, batch_size=BATCH_SIZE)\n",
    "    print(\"length of train_samples: \",data.total_len_train)\n",
    "    print(\"length of validation_samples: \",data.total_len_valid)\n",
    "    model = baseline_model()\n",
    "    print(model.summary())\n",
    "    try:\n",
    "        model.fit_generator(train_generator, samples_per_epoch=data.sum1, validation_data=(data.x_valid,data.y_valid), \n",
    "                            nb_val_samples=0, epochs=3, steps_per_epoch=data.sum1/BATCH_SIZE, verbose=1, ) #steps_per_epoch=STEPS\n",
    "    except KeyboardInterrupt as e:\n",
    "        print(str(e))\n",
    "    print(\"Model training, testing completed- Saving Model...\")\n",
    "    model.save(\"doodle_trial_model.h5\")\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\"\"\"\n",
    "Done loading the libraries\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "sum1:  120000\n",
    "validation data dimensions:  10114  ,  (10114, 4)\n",
    "length of train_samples:  84000.0\n",
    "length of validation_samples:  36000.0\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "input_5 (InputLayer)         (None, 6400)              0         \n",
    "_________________________________________________________________\n",
    "dense_9 (Dense)              (None, 100)               640100    \n",
    "_________________________________________________________________\n",
    "dense_10 (Dense)             (None, 4)                 404       \n",
    "=================================================================\n",
    "Total params: 640,504\n",
    "Trainable params: 640,504\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "None\n",
    "\"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['countrycode', 'drawing', 'recognized', 'timestamp', 'word', 'y', 'cv'], dtype='object')\n",
      "Counter({2: 2557, 0: 2549, 3: 2529, 1: 2479})\n"
     ]
    }
   ],
   "source": [
    "valid_df=pd.read_csv(os.path.join(TEST_DIR, 'test_k{}.csv.gz'.format(0)) )\n",
    "print(valid_df.columns)\n",
    "print(Counter(valid_df.y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "./<br>\n",
       "&nbsp;&nbsp;<a href='./__output__.json' target='_blank'>__output__.json</a><br>\n",
       "&nbsp;&nbsp;<a href='./doodle_trial_model.h5' target='_blank'>doodle_trial_model.h5</a><br>\n",
       "&nbsp;&nbsp;<a href='./__notebook__.ipynb' target='_blank'>__notebook__.ipynb</a><br>"
      ],
      "text/plain": [
       "./\n",
       "  __output__.json\n",
       "  doodle_trial_model.h5\n",
       "  __notebook__.ipynb"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLinks\n",
    "FileLinks('.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
